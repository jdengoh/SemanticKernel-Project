{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e26e83",
   "metadata": {},
   "source": [
    "# Semantic Kernal Quick Start Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e4bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b94d9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level for  semantic_kernel.kernel to DEBUG.\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logging.getLogger(\"kernel\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8780fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class LightsPlugin:\n",
    "    lights = [\n",
    "        {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False},\n",
    "        {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False},\n",
    "        {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True},\n",
    "    ]\n",
    "\n",
    "    # Kernel function to get the state of the lights\n",
    "    @kernel_function(\n",
    "        name=\"get_lights\",\n",
    "        description=\"Gets a list of lights and their current state\",\n",
    "    )\n",
    "    def get_state(\n",
    "        self,\n",
    "    ) -> str:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    # Kernel function to change the state of the lights\n",
    "    @kernel_function(\n",
    "        name=\"change_state\",\n",
    "        description=\"Changes the state of the light\",\n",
    "    )\n",
    "    def change_state(\n",
    "        self,\n",
    "        id: int,\n",
    "        is_on: bool,\n",
    "    ) -> str:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = is_on\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec9861a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837b61e",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94681d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.utils.logging import setup_logging\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Initialize the kernel\n",
    "    kernel = Kernel()\n",
    "\n",
    "    # Add Azure OpenAI chat completion\n",
    "    # chat_completion = AzureChatCompletion(\n",
    "    #     deployment_name=\"your_models_deployment_name\",\n",
    "    #     api_key=\"your_api_key\",\n",
    "    #     base_url=\"your_base_url\",\n",
    "    # )\n",
    "\n",
    "\n",
    "    chat_completion = OpenAIChatCompletion(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        ai_model_id=\"gpt-4o-mini\"\n",
    "    )\n",
    "    kernel.add_service(chat_completion)\n",
    "\n",
    "    # Set the logging level for  semantic_kernel.kernel to DEBUG.\n",
    "    setup_logging()\n",
    "    logging.getLogger(\"kernel\").setLevel(logging.DEBUG)\n",
    "\n",
    "    # Add a plugin (the LightsPlugin class is defined below)\n",
    "    kernel.add_plugin(\n",
    "        LightsPlugin(),\n",
    "        plugin_name=\"Lights\",\n",
    "    )\n",
    "\n",
    "    # Enable planning\n",
    "    execution_settings = AzureChatPromptExecutionSettings()\n",
    "    execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "    # Create a history of the conversation\n",
    "    history = ChatHistory()\n",
    "\n",
    "    # Initiate a back-and-forth chat\n",
    "    userInput = None\n",
    "    while True:\n",
    "        # Collect user input\n",
    "        userInput = input(\"User > \")\n",
    "\n",
    "        # Terminate the loop if the user says \"exit\"\n",
    "        if userInput == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Add user input to the history\n",
    "        history.add_user_message(userInput)\n",
    "\n",
    "        # Get the response from the AI\n",
    "        result = await chat_completion.get_chat_message_content(\n",
    "            chat_history=history,\n",
    "            settings=execution_settings,\n",
    "            kernel=kernel,\n",
    "        )\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Assistant > \" + str(result))\n",
    "\n",
    "        # Add the message from the agent to the chat history\n",
    "        history.add_message(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1655dd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > You have one light that is currently on (Chandelier) and two lights that are off (Table Lamp and Porch light).\n",
      "Assistant > It seems you didn't type anything. How can I assist you further?\n",
      "Assistant > The porch light is now turned on. If you need anything else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
